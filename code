from google.colab import files
import pandas as pd

# Upload the CSV
uploaded = files.upload()

# Replace with your actual file name (check in left file tab)
df = pd.read_csv("savant_data (7).csv")
df.head()

# Check available columns
print(df.columns.tolist())

# Build aggregation dictionary
agg_dict = {
    'launch_speed': 'mean',
    'launch_angle': 'mean',
    'swing_miss_percent': 'mean',
    'k_percent': 'mean',
    'bb_percent': 'mean',
    'babip': 'mean',
    'slg': 'mean',
    'iso': 'mean',
    'xba': 'mean',
    'xslg': 'mean',
    'barrels_per_bbe_percent': 'mean',
    'batter_run_value_per_100': 'mean',
    'bat_speed': 'mean',
    'obp': 'mean',
}

# If 'events' exists, count them â€” otherwise create it
if 'events' in df.columns:
    agg_dict['events'] = 'count'
else:
    df['events'] = 1
    agg_dict['events'] = 'sum'

# Group by player
df_grouped = df.groupby('player_name').agg(agg_dict).reset_index()
# Step 1: Define metrics
from sklearn.preprocessing import MinMaxScaler

positive_metrics = [
    'launch_speed', 'launch_angle', 'slg', 'babip',
    'barrels_per_bbe_percent', 'xslg', 'batter_run_value_per_100',
    'iso', 'xba', 'bat_speed', 'bb_percent','obp'
]
negative_metrics = ['whiff_percent', 'swing_miss_percent', 'k_percent']
all_metrics = positive_metrics + negative_metrics

# Step 2: Filter available metrics
available_metrics = [m for m in all_metrics if m in df_grouped.columns]

# Step 3: Normalize
scaler = MinMaxScaler()
normalized = scaler.fit_transform(df_grouped[available_metrics])
df_normalized = pd.DataFrame(normalized, columns=[f'{m}_norm' for m in available_metrics])
df_normalized['player_name'] = df_grouped['player_name']

# Step 4: Invert negative metrics (higher is worse, so we flip)
for m in negative_metrics:
    norm_col = f"{m}_norm"
    if norm_col in df_normalized.columns:
        df_normalized[norm_col] = 1 - df_normalized[norm_col]

# Step 5: Dynamic weighted rating function
def calculate_dynamic_rating(row):
    weights = {
        # Positive metrics
        'launch_speed_norm': 0.12,
        'launch_angle_norm': 0.07,
        'slg_norm': 0.10,
        'xslg_norm': 0.10,
        'obp_norm': 0.12,
        'babip_norm': 0.05,
        'barrels_per_bbe_percent_norm': 0.10,
        'batter_run_value_per_100_norm': 0.08,
        'iso_norm': 0.08,
        'xba_norm': 0.05,
        'bat_speed_norm': 0.07,
        'bb_percent_norm': 0.08,
        # Negative metrics
        'whiff_percent_norm': -0.05,
        'swing_miss_percent_norm': -0.08,
        'k_percent_norm': -0.10,

    }

   # Define the dynamic rating calculation
def calculate_dynamic_rating(row):
    # Initialize weights more carefully
    weights = {
        'launch_speed_norm': 0.10,
        'launch_angle_norm': 0.07,
        'swing_miss_percent_norm': -0.07,
        'k_percent_norm': -0.07,
        'bb_percent_norm': 0.08,
        'babip_norm': 0.08,
        'slg_norm': 0.10,
        'iso_norm': 0.08,
        'xba_norm': 0.08,
        'xslg_norm': 0.08,
        'barrels_per_bbe_percent_norm': 0.10,
        'batter_run_value_per_100_norm': 0.08,
        'bat_speed_norm': 0.07,
        'obp_norm': 0.10
    }

    # === Dynamic Adjustments based on player profile ===
    if row.get('swing_miss_percent', 0) < 20 and row.get('k_percent', 0) > 20:
        weights['swing_miss_percent_norm'] -= 0.05
        weights['k_percent_norm'] += 0.02

    if row.get('slg', 0) > 0.450:
        weights['slg_norm'] += 0.10
        weights['launch_speed_norm'] += 0.01

    if row.get('obp', 0) > 0.350:
        weights['obp_norm'] += 0.10
    elif row.get('obp', 0) < 0.280:
        weights['obp_norm'] -= 0.05

    if row.get('launch_speed', 0) > 92:
        weights['launch_speed_norm'] += 0.05
    elif row.get('launch_speed', 0) < 87:
        weights['launch_speed_norm'] -= 0.05

    if row.get('barrels_per_bbe_percent', 0) > 10:
        weights['barrels_per_bbe_percent_norm'] += 0.03

    if row.get('launch_angle', 0) < 5 or row.get('launch_angle', 0) > 30:
        weights['launch_angle_norm'] -= 0.03

    if row.get('babip', 0) < 0.250:
        weights['babip_norm'] -= 0.05
    elif row.get('babip', 0) > 0.290:
        weights['babip_norm'] += 0.08

    if row.get('xslg', 0) > 0.450:
        weights['xslg_norm'] += 0.10

    if row.get('batter_run_value_per_100', 0) < 0.5:
        weights['batter_run_value_per_100_norm'] -= 0.05

    # === Normalize weights so they sum to ~1 ===
    total_weight = sum(abs(w) for w in weights.values())
    weights = {k: v / total_weight for k, v in weights.items()}

    # === Final Rating Computation ===
    rating = sum(row.get(metric, 0) * weight for metric, weight in weights.items())

    # Keep it between 0 and 10
    return max(0, min(rating * 10, 10))


# Step 6: Combine raw + normalized stats
df_combined = df_grouped.merge(df_normalized, on='player_name')

# Step 7: Calculate dynamic ratings
df_combined['rating'] = df_combined.apply(calculate_dynamic_rating, axis=1)

# Step 8: View top players
top_players = df_combined[['player_name', 'rating']].sort_values(by='rating', ascending=False).head(10)
print(top_players)
def compare_players(player_names, df_combined):
    # Raw and normalized metrics to include
    metrics = [
        'launch_speed', 'launch_angle', 'events',
        'hard_hit_percent', 'barrel_batted_rate',
        'k_percent', 'bb_percent', 'xwOBA', 'xBA', 'xSLG',
        'launch_speed_norm', 'launch_angle_norm', 'rating'
    ]

    # Filter for selected players
    df_filtered = df_combined[df_combined['player_name'].isin(player_names)].set_index('player_name')

    # Only keep available columns
    cols_to_use = [col for col in metrics if col in df_filtered.columns]

    # Return comparison DataFrame
    return df_filtered[cols_to_use]
# Example use:
compare_players(['Harper, Bryce', 'Alonso, Pete'], df_combined)
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Ensure df_combined already includes merged + normalized stats
df_combined = df_grouped.merge(df_normalized, on='player_name')

# Calculate rating for each player
df_combined['rating'] = df_combined.apply(calculate_dynamic_rating, axis=1)

# Define features to be used
features = [
    'launch_speed', 'launch_angle', 'slg', 'babip', 'barrels_per_bbe_percent',
    'xslg', 'batter_run_value_per_100', 'iso', 'xba', 'bat_speed', 'bb_percent',
    'whiff_percent', 'swing_miss_percent', 'k_percent'
]

# Only use available features
features = [f for f in features if f in df_combined.columns]

# Drop rows with missing values
df_model = df_combined.dropna(subset=features + ['rating'])

# Input features and target variable
X = df_model[features]
y = df_model['rating']
# Split the dataset (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
# You can tweak these hyperparameters to improve performance
model = GradientBoostingRegressor(
    n_estimators=200,       # Number of trees
    learning_rate=0.1,      # Step size shrinkage
    max_depth=4,            # Max depth of trees
    random_state=42
)

model.fit(X_train, y_train)
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Predict on the test set
y_pred = model.predict(X_test)

# Print evaluation metrics
print("R^2 Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))  # Manually compute RMSE
# Reset index before splitting
df_model = df_model.reset_index(drop=True)
X = df_model[features]
y = df_model['rating']

# Train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Optional: View predicted vs actual for test set
comparison = pd.DataFrame({
    'player_name': df_model.loc[y_test.index, 'player_name'],
    'actual_rating': y_test.values,
    'predicted_rating': y_pred
})

print(comparison.head(100))
